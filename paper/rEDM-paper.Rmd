---
title: "`rEDM`: An **R** package for Empirical Dynamic Modeling and Convergent Cross Mapping"
preprint: false
author: 
  - name: Hao Ye
    affiliation: 1
    corresponding: true
    email: haoye@ufl.edu
  - name: Adam T. Clark
    affiliation: 2
  - name: Ethan R. Deyle
    affiliation: 3
  - name: George Sugihara
    affiliation: 3
affiliation:
  - code: 1
    address: Department of Wildlife Ecology and Conservation, University of Florida, 110 Newins-Ziegler Hall, PO Box 110430, Gainesville, FL USA 32611-0430
  - code: 2
    address: German Centre for Integrative Biodiversity Research (iDiv)
  - code: 3
    address: Scripps Institution of Oceanography, University of California San Diego, 9500 Gilman Drive MC 0202, La Jolla, CA USA 92093-0202
abstract: "
  1. Modeling the ecological processes that underlie time series observations can often be challenging because of uncertainty about the actual equations, or because governing equations depend on state variables that are unknown or unobserved.
  
  2. Empirical dynamic modeling (EDM) is an emerging framework for modeling dynamic systems, based on the mathematical theory of attractor reconstruction. These methods can be successful even when there is only one observed time series from a multivariate system.
  
  3. The `rEDM` package contains a number of functions for EDM, including methods for forecasting, inferring causal relationships among time series, and measuring changing interactions.
  
  4. `rEDM` facilitates the use of EDM approaches for understanding ecosystems, through the creation of models more robust to observations and structural uncertainty about equations, testing for causal relationships between observed variables, or exploratory analysis of time series datasets that can lead to hypothesis generation."
header-includes: >
  \usepackage{amsmath}
  \usepackage{setspace}
  \doublespacing
bibliography: refs.bib
output: 
  rticles::peerj_article:
    base_format: bookdown::pdf_document2 # for using \@ref()
    # base_format: rmarkdown::pdf_document 
---
### Keywords {-}
causal inference, empirical dynamics, forecasting, time series

### Running Title {-}
`rEDM`: Empirical Dynamic Modeling in R

\newpage

# Introduction

In ecology, models are approximations of reality that are used for descriptive, explanatory, or predictive purposes.

One common application of models is to describe how important ecological variables, such as abundance, change over time, and in relation to each other. Historically, such models have been based on parametric equations that are derived from theory, and then tested in laboratory or simulation settings. However, there are many situations where explicit mathematical formulations are impractical: the exact mechanisms may be unknown or very complex; and the available data may be too limited or noisy to fully parameterize these equations.

These obstacles can hinder the effective usage of ecological models; for example, effort may be expended on increased monitoring and observation, yet yield little improvement in understanding or predictive power, because of inappropriate or inflexible mathematical hypotheses. In contrast, phenomenological models are highly flexible alternatives, which operate by inferring patterns and associations from the data (instead of relying on hypothesized equations). Here, we review the theoretical background for reconstructing dynamic rules from time series data, an approach known as "Empirical Dynamic Modeling (EDM)" and the functionality of the `rEDM` package in R [XXX]. 

EDM combines the mathematical theory of attractor reconstruction [@Takens_1981; @Sauer_1991; @Casdagli_1991; @Deyle_2011] and simple methods for function approximation [@Sugihara_1990; @Sugihara_1994] in order to model complex systems using limited time series observations. Thus, it can be a powerful method for improving ecological modeling. Moreover, becasue EDM operates with minimal assumptions, it is particularly suitable for studying systems that exhibit non-equilibrium dynamics and nonlinear state-dependent behavior (i.e. where interactions change over time and as a function of the system state).

<!-- which are intended for nonlinear dynamic systems that can prove problematic for traditional modeling approaches. -->
<!-- especially problematic when used for predictive purposes () -->

<!-- Empirical models, which infer patterns and associations from the data (instead of using hypothesized equations), represent an alternative and highly flexible approach. Here, we review the theoretical background for empirical dynamic modeling (EDM) and the functionality of the `rEDM` package, which are intended for nonlinear dynamic systems that can prove problematic for traditional modeling approaches. -->

<!-- The basic goal underlying EDM is to reconstruct the behavior of dynamic systems using time series data. This approach is based on mathematical theory developed initially by [@Takens_1981], and subsequently expanded [@Sauer_1991; @Casdagli_1991; @Deyle_2011]. Because these methods operate with minimal assumptions, they are particularly suitable for studying systems that exhibit non-equilibrium dynamics and nonlinear state-dependent behavior (i.e. where interactions change over time and as a function of the system state). -->

# Empirical Dynamic Modeling

## Time Series as Observations of a Dynamic System

The essential concept underlying EDM is that time series are sequential observations of a *dynamic system*. Briefly, a dynamic system consists of a *state space* and *dynamical rules* for how the system changes over time. At any point in time, the state of the system is represented as a point in the state space (Figure \@ref(fig:time-series-projection)), whose axes are the fundamental state variables. The rules for the system are typically represented as governing equations for how the system state will change based on the current state (i.e. the combined values of the state variables).

For example, the state variables for a phytoplankton experiment would be the population abundance and environmental factors: temperature, nutrients, and light. The dynamical rules are the equations for growth and nutrient uptake, along with the experimental settings for how temperature, light, and nutrient input are controlled.

A time series for a state variable can be generated by projecting from the state space to the corresponding coordinate axis. For example, in Figure \@ref(fig:time-series-projection), the states of the canonical Lorenz Attractor [-@Lorenz_1963] are projected to the $x$-axis, creating a time series of variable $x$. In many cases, time series correspond to different state variables. More generally, however, a time series can be any function of one or more state variables (which may defy convenient geometrical representation).

```{r time-series-projection, out.width = "80%", fig.cap = "Time Series Projection from the Lorenz Attractor", echo = FALSE}
knitr::include_graphics("figure_1.pdf")
```

The conventional approach to modeling a dynamic system is to simulate it, using the governing equations, and instantiating the simulation using observations of the state variables. Uncertainity about vital parameters is addressed through fitting the equations to observational data; and uncertainty about the form of the equations is addressed using model selection. However, this approach assumes that the state variables are known, that they are observed sufficiently, and that the governing equations are known or can be reasonably approximated from theory. This can be problematic when seeking to model time series drawn from complex, nonlinear systems.

## Attractor Reconstruction / Takens' Theorem

In the EDM framework, these issues are addressed using the mathematical theory of attractor reconstruction [@Takens_1981; @Sauer_1991; @Casdagli_1991; @Deyle_2011]. The essential idea is that the state of the system can be represented using lags of a single time series, instead of using all of the state variables. For example, in the canonical Lorenz Attractor, the position at time $t$ is typically represented as $\left(x_t, y_t, z_t \right)$, where $x$, $y$, and $z$ are the state variables (Figure \@ref(fig:time-series-projection)). However, we can also represent the state using only lags of $x$: $\mathbf{x}_t = \left( x_t, x_{t-\tau}, \dots, x_{t-(E-1)\tau} \right)$, where $E$ is the embedding dimension, or number of lags.

```{r attractor-reconstruction, out.width = "80%", fig.cap = "Attractor Reconstruction from 3 Lagged Coordinates", echo = FALSE}
knitr::include_graphics("figure_2.pdf")
```

This approach, known as *attractor reconstruction*, preserves the mathematical properties of the original system, provided that a sufficient number lags are used [@Takens_1981]. More specifically, points in the reconstructed space, $\mathbf{x}_t$, map smoothly to points in the original state space ("smooth" meaning that nearby points in the reconstructed space also map to nearby points in the original state space). This relationship makes it possible to model a dynamic system using just a single time series.

For example, to describe the motion of $x$ in the Lorenz Attractor, the typical approach is to use a function of all of the state variables (i.e. $x_{t+1} = F\left(x_t, y_t, z_t\right))$. However, we can instead use an alternative formulation based on the reconstructed state space: $x_{t+1} = F^\prime\left(\mathbf{x}_t\right) = F^\prime\left(x_t, x_{t-\tau}, \dots, x_{t-(E-1)\tau} \right)$. Figure \@ref(fig:attractor-reconstruction) demonstrates this point with a reconstruction that uses 3 lags of $x$, which shows a visual correspondence with the original Lorenz attractor.

In practice, the application of this technique requires selecting an appropriate time lag, $\tau$, embedding dimension, $E$, and methods for inferring the dynamical model, $F^\prime$. In the next section, we demonstrate how the **rEDM** software package can be used to accomplish these tasks, with applications for forecasting [@Sugihara_1990], testing for nonlinear behavior [@Sugihara_1994], and testing whether two time series variables belong to the same system and might be causally related [@Sugihara_2012]. 

# The **rEDM** package

## Selecting the embedding dimension using simplex projection

As mentioned previously, the mathematical theory of *attractor reconstruction* requires a sufficient number of lags [@Takens_1981]. If the number of lags is too few, then the reconstructed space ($\textbf{x}_t$) no longer maps smoothly to the original state space. Intuitively, a sufficient number of lags are required to capture the influence of all of the state variables in the original state space (see also [@Whitney_1936]). Practically, the number of lags, or the embedding dimension, is determined empirically using the data; here, we use forecast skill as the metric for selecting an optimal embedding dimension.

### Example

In this example, time series are generated from a simulation of the tent map, a discrete-time dynamic system where a sequence, $x_t$, on the interval $[0, 1]$ is iterated according to:

\begin{equation*}
x_{t+1} = \begin{cases}
2x_t, & x_t < \frac{1}{2}\\
2(1-x_t), & x_t \ge \frac{1}{2}
\end{cases}
\end{equation*}

The first difference of the tentmap time series are included with the **rEDM** package:

```{r load package}
library(rEDM)

data(tentmap_del)
str(tentmap_del)
```

Here, the `tentmap_del` variable is a numeric vector with `r length(tentmap_del)` values. As one of the accepted input formats, no further processing of the data is required (see section XXX for more information about data formats).

To select the portions of the time series to be used for fitting the model and then testing the model, we define `lib` and `pred` variables:

```{r define lib and pred}
lib <- c(1, 100)
pred <- c(201, 500)
```

Here, `lib <- c(1, 100)` indicates that rows 1 through 100 of the input data are used for fitting the model, and `pred <- c(201, 500)` indicates that rows 201 through 500 of the input data are used for testing the model.

Since the time series come from a discrete map, a time lag of `tau = 1` is appropriate (and is the default for functions in the **rEDM** package). In addition, because we know the dynamics are fairly simple, the default range of $E = 1:10$ for the embedding dimension will suffice. More generally, in the absence of prior knowledge about the system, a reasonable range of values is 1 through $sqrt(n)$ with $n$ being the length of the time series [REF]).

```{r simplex on tentmap}
simplex_output <- simplex(time_series = tentmap_del, 
                          lib = lib, pred = pred, 
                          tau = 1, E = 1:10)
```

Here, the `simplex()` function makes forecasts using the parameters just described, and employs Simplex Projection to infer the dynamical model $F^\prime$ from the data using nearest neighbor approximation [@Sugihara_1990].

```{r simplex output}
str(simplex_output)
```

The output is a data.frame with rows for each individual model run, corresponding to the unique combination of model parameters. Here, there are `r NROW(simplex_output)` rows for each value of the embedding dimension that we tried. The columns are the model parameters: `E`, embedding dimension; `tau`, time lag between successive dimensions; `tp`, time to prediction; and `nn`, number of nearest neighbors; and forecast statistics: `num_pred`, the number of predictions made; `rho`, Pearson's correlation coefficient between predictions and observations; `mae`, mean absolute error of predictions; `rmse`, root mean squared error of predictions; `perc`, the percent of predictions that are the same sign as observations; and `p_val`, the p-value for `rho` being significantly greater than 0, using Fisher's transformation [-@Fisher_1915]. For the purpose of comparison, the same forecast statistics are then computed for a naive constant predictor, where $\hat{x}_{t+tp} = x_t$.

(ref:simplex-caption) Forecast skill (rho) vs. Embedding Dimension (E) for the `tentmap_del` time series. A peak at E = 2 indicates that 2 total lags are optimal for reconstructing the dynamics. 

```{r tentmap-simplex, fig.width = 4.5, fig.height = 3, fig.cap = '(ref:simplex-caption)', echo = FALSE}
par(mar = c(4, 4, 1, 1), mgp = c(2.5, 1, 0))
plot(rho ~ E, data = simplex_output, type = "l",  
     xlab = "Embedding Dimension (E)", ylab = "Forecast Skill (rho)")
```

Figure \@ref(fig:tentmap-simplex) shows that forecast skill peaks at E = 2, indicating that the dynamics of the `tentmap_del` time series are best represented using 2 total lags. *Notably, this optimal embedding dimension does not necessarily correspond to the dimensionality of the original system.* Forecast skill is affected by factors such as observational noise, process error, and time series length. Thus, the optimal embedding dimension should be interpreted as a practical measure that is dependent on properties of the data.

## Identifying Nonlinearity

One concern is that the ability to make short-term forecasts could result just from temporal autocorrelation in the time series. To make forecasts, we infer the function, $F^\prime$, that maps from a set of lags of a time series to its next value; consequently, this approach will also succeed when used to model temporally autocorrelated data (i.e. red noise). To distinguish between time series that contain deterministic nonlinear dynamics and red noise, we apply S-maps [@Sugihara_1994].

Whereas Simplex Projection approximates $F^\prime$ using nearest neighbors in the reconstruction space [@Sugihara_1990], the S-map uses local linear maps that are fitted separately for each forecast [@Sugihara_1994]. When fitting a local linear map, the weighting assigned to points is controlled by the nonlinear tuning parameter, $\theta$. When $\theta = 0$, all points receive equal weights, and the local linear map at any point in the reconstruction space is the same as a single global linear map. In contrast, for $\theta > 0$, nearby points receive larger weights, such that the local linear map depends more on similar ecosystem states (typically determined by Euclidean distance between different $\textbf{x}_t$). Larger values of $\theta$ allow the forecast function, $F^\prime$ to be more "wiggly", similar to the weight function in a LOESS [REF].

This tuning parameter enables distinguishing between red noise and deterministic nonlinear dynamics. For autoregressived red noise, a single global map ($\theta = 0$ should produce the best forecasts, becauses it is fit to all of the data points, and therefore minimizes the effects of observational noise. However, for deterministic nonlinear dynamics, the true forecast function, $F^\prime$, will vary as a function of system state, and so forecast skill will improve when $\theta > 0$.

### Example

The function `s_map()` implements the S-map method. Following from the previous example, we use the tent map time series, and fix `E = 2` based on the results from simplex projection. The default values for `tau` and `tp` are appropriate here, so we do not specify them. However, if we had used different values when determining the optimal embedding dimension using simplex projection, we would want to use the same values here. The default values for `theta` range from `0` to `8`, and are also suitable for our purposes.

```{r smap for tentmap}
smap_output <- s_map(time_series = tentmap_del, 
                     lib = lib, pred = pred, 
                     E = 2)
```

Again, the results are a data.frame with rows for each model run, and columns for the model parameters and forecast statistics. Note that the default value for `num_neighbors` is `0`: this parameter controls how many of the nearest points the S-map is allowed to be fit to (which can ease calculations when there are a large number of data points). A value of 0 tells the S-map to use all the points, and to use $\theta$ to control the weighting assigned to individual points.

(ref:smap-caption) Forecast skill (rho) vs. Nonlinearity (theta) for the `tentmap_del` time series. Increased forecast skill for theta > 0 suggests nonlinear dynamics in the data.

```{r tentmap-smap, fig.width = 4.5, fig.height = 3, fig.cap = '(ref:smap-caption)', echo = FALSE}
par(mar = c(4, 4, 1, 1), mgp = c(2.5, 1, 0))
plot(smap_output$theta, smap_output$rho, type = "l",
     xlab = "Nonlinearity (theta)", ylab = "Forecast Skill (rho)")
```

Figure \@ref(fig:tentmap-smap) shows that forecast skill substantially improves as $\theta$ increases, indicating the presence of nonlinear dynamics. Typically, we would also expect forecast skill to decrease as $\theta$ continues to increase, because the local linear map will change drastically for each individual point, overfitting the data. However, because data in this example are observed without any noise, the S-map is able to converge closer to the true function as $\theta$ increases.

If we simulated additive observational error and re-run the S-map analysis, the result is a plot that is more typical of real data (Figure \@ref(fig:tentmap-smap-noise)).

```{r rho vs theta with noise}
ts_err <- tentmap_del + rnorm(length(tentmap_del), 
                              sd = sd(tentmap_del) * 0.2)
smap_output_err <- s_map(time_series = ts_err, 
                         lib = lib, pred = pred, 
                         E = 2)
```

(ref:smap-noise-caption) With added observational noise, forecast skill (rho) increases and then decreases with theta.

```{r tentmap-smap-noise, fig.width = 4.5, fig.height = 3, fig.cap = '(ref:smap-noise-caption)', echo = FALSE}
par(mar = c(4, 4, 1, 1), mgp = c(2.5, 1, 0))
plot(smap_output_err$theta, smap_output_err$rho, type = "l",
     xlab = "Nonlinearity (theta)", ylab = "Forecast Skill (rho)")
```

## Generalized Takens's Theorem

In addition to representing the system state using lags of a single time series [@Takens_1981], we can use different time series observed from the same system [@Sauer_1991; @Deyle_2011]. As a practical point, these *multivariate* representations can be more effective when data are limited and noisy, or there are stochastic drivers in the system that need to be explicitly included. For more details, see @Casdagli_1991.

In **rEDM**, the `block_lnlp()` function generalizes the `simplex()` and `s_map()` functions: the reconstruction space can be formed from any combination of coordinates, and both Simplex Projection and S-map are supported.

The data format for `block_lnlp()` is a matrix or data.frame, where each column is a time series and each row is a temporal "slice" of contemporaneous observations. A reconstruction is defined by specifying the columns to use as coordinates (the `columns` argument) and the column to be forecast (the `target_column` argument). If lags of a time series are desired as coordinates, they need to be pre-computed as separate columns (e.g. via the `make_block()` function).

### Example

We begin with an example dataset from a coupled 3-species model system.

```{r load block_3sp data}
data(block_3sp)
str(block_3sp)
```

`block_3sp` is a 10-column data-frame, consisting of `time`, and 3 lags for each of the variables: unlagged (`_t-1`), lag-1 (`_t-1`), and lag-2 (`_t-2`). Note that the lagged columns begin with `NA` values because they correspond to missing observations of the variables (e.g. `x` at time `0`). Points that contain missing values are automatically excluded from fitting and forecasting (see section [SEC] for more details).

Columns can be referred to by either numerical index or column name. Following R convention, `1` refers to the first column, with the option of setting `first_column_time = TRUE` to skip over a time column if it is the first column.

```{r block-lnlp for block-3sp, warning = FALSE}
lib <- c(1, 100)
pred <- c(101, 200)

cols <- c(1, 2, 4)
target <- 1

block_lnlp_output <- block_lnlp(block_3sp, lib = lib, pred = pred,
                                columns = cols, target_column = target,
                                first_column_time = TRUE, 
                                stats_only = FALSE)
```

Columns can also be referred to by name. Although it is not necessary to specify `first_column_time = TRUE`, doing so will use the values in the time column to label predictions.

```{r}
block_lnlp_output_alt <- block_lnlp(block_3sp, lib = lib, pred = pred,
                                    columns = c("x_t", "x_t-1", "y_t"), 
                                    target_column = "x_t", 
                                    first_column_time = TRUE, 
                                    stats_only = FALSE)

# test for equality
stopifnot(identical(block_lnlp_output, block_lnlp_output_alt))
```

As in `simplex()` and `s_map()`, the default value for the `tp` argument is 1, indicating that predictions are 1-step ahead. In other words, the reconstructed space consists of points, whose coordinates are the values in a single row of the input data, with columns selected by `columns`; and these points are being mapped to the values in the subsequent row and in the `target_column` column. *In some cases, the data may be formatted such that the value to be predicted is another variable in the same row, and so `tp` should be set to 0.*

```{r}
str(block_lnlp_output)
```

With `stats_only = FALSE`, the output also includes a list column (`model_output`), where each element is a data.frame that contains observed and predicted values for each model run. We can extract out these values to show how individual predictions match up against observed values.

```{r}
list_of_model_predictions <- block_lnlp_output$model_output
first_model_predictions <- list_of_model_predictions[[1]]

observed <- first_model_predictions$obs
predicted <- first_model_predictions$pred
```

(ref:block-pred-vs-obs-caption) The predictions of variable $x$ against the observed values of $x$ from running `block_lnlp()`. Perfect predictions (with no error) will fall on the 1:1 line (dashed).

```{r block-pred-vs-obs, fig.width = 4, fig.height = 4, fig.cap = '(ref:block-pred-vs-obs-caption)', echo = FALSE}
par(pty = "s")
plot_range <- range(c(observed, predicted), na.rm = TRUE)
plot(observed, predicted, xlim = plot_range, ylim = plot_range,
     xlab = "Observed", ylab = "Predicted", asp = 1)
abline(a = 0, b = 1, lty = 2, col = "blue")
```

## S-map Coefficients

As described in [@Deyle_2016], the S-map coefficients from the appropriate multivariate embedding can be interpreted as dynamic, time-varying interaction strengths. We demonstrate this approach for the same 3-species simulation as above, using `x`, `y`, and `z` as the coordinates to predict `x`.

```{r 3-species s-map coefficients example}
data(block_3sp)
lib <- c(1, 100)
pred <- c(101, 200)

cols <- c("x_t", "y_t", "z_t")
target <- "x_t"

block_smap_output <- block_lnlp(block_3sp, lib = lib, pred = pred,
                                columns = cols, target_column = target,
                                method = "s-map", theta = 2,
                                stats_only = FALSE, first_column_time = TRUE,
                                save_smap_coefficients = TRUE, silent = TRUE)
```

The `smap_coefficients` column of the output is a list-column with the data.frames for the S-map coefficients of each model. Since we have just have one model, we just want the first element of that list. The result is a data.frame with 200 rows (for each prediction) and 4 columns (for each of the 3 predictors and a constant).

```{r get coefficients}
smap_coeffs <- block_smap_output$smap_coefficients[[1]]
str(smap_coeffs)
```

Here, we plot the time series for the observed (solid) and predicted (dashed) values of `x` in the top panel; and the inferred interactions (s-map coefficients) for the influence of `x`, `y`, and `z` on future values of `x` in the remaining 3 panels.

```{r, echo = FALSE}
par(mfrow = c(4, 1), mar = c(2, 4, 1, 1), oma = c(0, 0, 0, 0),
    mgp = c(2.5, 1, 0))
```

```{r smap coefficients plot, fig.width = 6, fig.height = 7}
predictions <- block_smap_output$model_output[[1]]
t <- predictions$time

plot(t, predictions$obs, type = "l", col = "black", ylab = "x", xlab = "")
lines(t, predictions$pred, lty = 2)
legend("topright", legend = c("observed", "predicted"), lty = c(1, 2), bty = "n")

plot(t, smap_coeffs[, 1], type = "l", col = "red", ylab = "effect of x", xlab = "")
plot(t, smap_coeffs[, 2], type = "l", col = "blue", ylab = "effect of y", xlab = "")
plot(t, smap_coeffs[, 3], type = "l", col = "magenta", ylab = "effect of z", xlab = "")
```

```{r, echo = FALSE}
par(mfrow = c(1, 1), mar = c(4, 4, 1, 1))
```

## Causality Inference and Cross Mapping

One of the corollaries to Takens' Theorem is that multiple reconstructions not only map to the original system, but also to each other. Consider two variables, $x$ and $y$ that interact in a dynamic system. The univariate reconstructions based on lags of $x$ ($\mathbf{M}_x$) or $y$ ($\mathbf{M}_y$) are each capable of uniquely identifying system states. Thus, the reconstructed states of $\mathbf{M}_x$ map to reconstructed states of $\mathbf{M}_y$ (Figure \@ref(fig:cross-mapping)). This suggests a way to test whether $x$ and $y$ interact in the same system, by testing for a mapping between $\mathbf{M}_x$ and $\mathbf{M}_y$. Practically this is done by testing the predictive skill for the mapping from $\mathbf{M}_x$ to $y$ and from $\mathbf{M}_y$ to $x$.


```{r cross-mapping, out.width = "80%", fig.cap = "Cross Mapping Between Reconstructions of the Lorenz Attractor", echo = FALSE}
knitr::include_graphics("figure_3.pdf")
```

Furthermore, in the case of unidirectional causality, e.g. $x$ causes $y$, but $y$ does not cause $x$, we would only expect cross mapping to be successful in one direction. Somewhat counterintuitively, if a causal variable ($x$) leaves a signature on the affected variable ($y$), then it is possible to map from $\mathbf{M}_y$ to $\mathbf{M}_x$, but not vice-versa.

In essence, $\mathbf{M}_y$ must have complete information about $y$, which means it must include information about all its causes, including $x$. However, because $x$ behaves independently of $y$, the reconstruction $\mathbf{M}_x$ may be missing information about $y$, preventing a *complete* cross mapping from $\mathbf{M}_x$ to $y$.

To be more precise, although $x$ has incomplete information about $y$, it does have a causal influence on $y$, and there will likely be some predictive skill in the mapping from $\mathbf{M}_x$ to $y$. However, this will be limited to the statistical association between $x$ and $y$ and will generally not improve with more data.

In contrast, the mapping from $\mathbf{M}_y$ to $x$ is expected to become complete with more data. This convergence is a critical property for inferring causality, and can be tested by measuring the cross mapping skill when using different amounts of data to reconstruct $\mathbf{M}_y$. For a more detailed description of using cross mapping to infer causation, see [@Sugihara_2012; @Ye_2015a].

## Convergent Cross Mapping (CCM)

In **rEDM**, convergent cross mapping is implemented as the `ccm()` function, which provides a wrapper to compute cross map skill for different subsamples of the data. In the following example, we reproduce the analysis from [@Sugihara_2012] to identify causality between anchovy landings in California and Newport Pier sea-surface temperature. For this example, we use the previously identified embedding dimension of `E = 3`.

To identify convergence, we compute cross-map skill (Pearson's correlation, $\rho$ between observed and predicted values) over many random subsamples of the time series. The `lib_sizes` argument specifies the size of the library set, and `num_samples` specifies the number of subsamples generated at each library size. `random_libs` and `replace` specify how the subsamples will be generated. Here, setting both to `TRUE` enables random sampling with replacement.

```{r sardine anchovy ccm, warning = FALSE}
data(sardine_anchovy_sst)
anchovy_xmap_sst <- ccm(sardine_anchovy_sst, E = 3,
                        lib_column = "anchovy", target_column = "np_sst",
                        lib_sizes = seq(10, 80, by = 10), num_samples = 100,
                        random_libs = TRUE, replace = TRUE, silent = TRUE)
sst_xmap_anchovy <- ccm(sardine_anchovy_sst, E = 3,
                        lib_column = "np_sst", target_column = "anchovy",
                        lib_sizes = seq(10, 80, by = 10), num_samples = 100,
                        random_libs = TRUE, replace = TRUE, silent = TRUE)
str(anchovy_xmap_sst)
```

The output is a data.frame with statistics for each model run (in this case, 100 models at each of 8 library sizes). To interpret the results, we aggregate the cross map performance at each library size using the `ccm_means()` function, which computes a mean value at each unique `lib_size`. Because average cross map skill less than 0 means there is no prediction skill, (predictions should not be anticorrelated with observations), we set negative values to 0 when plotting.

```{r sardine anchovy ccm plot}
a_xmap_t_means <- ccm_means(anchovy_xmap_sst)
t_xmap_a_means <- ccm_means(sst_xmap_anchovy)

plot(a_xmap_t_means$lib_size, pmax(0, a_xmap_t_means$rho), type = "l", col = "red",
     xlab = "Library Size", ylab = "Cross Map Skill (rho)", ylim = c(0, 0.25))
lines(t_xmap_a_means$lib_size, pmax(0, t_xmap_a_means$rho), col = "blue")
legend(x = "topleft", legend = c("anchovy xmap SST", "SST xmap anchovy"),
       col = c("red", "blue"), lwd = 1, bty = "n", inset = 0.02, cex = 0.8)
```

# Authorsâ€™ contributions {-}

H.Y. and G.S. conceived the package; H.Y. designed the package, with assistance from A.T.C. and E.R.D. on coding, testing, and documentation; All authors were involved with writing and editing the paper.

# Acknowledgements {-}

**rEDM** is the latest incarnation of EDM code. Past versions have been developed by George Sugihara, Alan Trombla, Richard Penner, Victor Wong, Martin Casdagli, Jerome Cartagena, Mohsen Azarbayejani, Ava Pierce, Jennifer Trezzo, and Hao Ye.

We thank Jun Cai, Jane Cowles, Yair Daon, Andrew Edwards, Oliver Keyes, Steve Munch, James Stagge, Masayuki Ushio, and Ethan White, for their suggestions and contributions to the package.

This project is supported by a Gordon and Betty Moore Foundation Data-Driven Discovery Initiative through Grant GBMF4563 (to Ethan P. White), NSF grant DEB-1655203 (GS), NSF grant DBI-1667584 (GS), U.S. Department of Defense Strategic Environmental Research and Development Program 15 RC-2509 (GS), Lenfest Ocean Program award 00028335 (GS), the Deutsche Bank-Jameson Complexity Studies Fund (GS), the Sugihara Family Trust (GS), the Leslie and John McQuown Gift and the McQuown Chair in Natural Sciences, UCSD (GS).

Data collection for the Cedar Creek LTER was funded by NSF grant DEB-9411972 (to G. David Tilman), DEB-0080382 (to G. David Tilman), DEB-0620652 (to G. David Tilman), and DEB-1234162 (to Eric Seabloom).

# Data accessibility {-}

Example datasets used here are included in the `rEDM` package, which is hosted at https://github.com/ha0ye/rEDM and also archived on Zenodo [@Ye_2018].

# Appendix

*Note that if the code detects any overlap in the lib and pred, it will prevent a vector from becoming its own neighbor by enabling leave-one-out cross-validation and outputting a warning message.*


# References
