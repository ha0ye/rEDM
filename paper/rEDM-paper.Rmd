---
title: "`rEDM`: An **R** package for Empirical Dynamic Modeling and Convergent Cross Mapping"
preprint: false
author: 
  - name: Hao Ye
    affiliation: 1
    corresponding: true
    email: haoye@ufl.edu
  - name: Adam T. Clark
    affiliation: 2
  - name: Ethan R. Deyle
    affiliation: 3
  - name: George Sugihara
    affiliation: 3
affiliation:
  - code: 1
    address: Department of Wildlife Ecology and Conservation, University of Florida, 110 Newins-Ziegler Hall, PO Box 110430, Gainesville, FL USA 32611-0430
  - code: 2
    address: German Centre for Integrative Biodiversity Research (iDiv)
  - code: 3
    address: Scripps Institution of Oceanography, University of California San Diego, 9500 Gilman Drive MC 0202, La Jolla, CA USA 92093-0202
abstract: "
  1. Modeling the ecological processes that underlie time series observations can often be challenging because of uncertainty about the actual equations, or because governing equations depend on state variables that are unknown or unobserved.
  
  2. Empirical dynamic modeling (EDM) is an emerging framework for modeling dynamic systems, based on the mathematical theory of attractor reconstruction. These methods can be successful even when there is only one observed time series from a multivariate system.
  
  3. The `rEDM` package contains a number of functions for EDM, including methods for forecasting, inferring causal relationships among time series, and measuring changing interactions.
  
  4. `rEDM` facilitates the use of EDM approaches for understanding ecosystems, through the creation of models more robust to observations and structural uncertainty about equations, testing for causal relationships between observed variables, or exploratory analysis of time series datasets that can lead to hypothesis generation."
header-includes: >
  \usepackage{amsmath}
  \usepackage{setspace}
  \doublespacing
bibliography: refs.bib
output: 
  rticles::peerj_article:
    base_format: bookdown::pdf_document2 # for using \@ref()
    # base_format: rmarkdown::pdf_document 
---
### Keywords {-}
causal inference, empirical dynamics, forecasting, time series

### Running Title {-}
`rEDM`: Empirical Dynamic Modeling in R

\newpage

# Introduction

In ecology, models are approximations of reality that are used for descriptive, explanatory, or predictive purposes.

One common application of models is to describe how important ecological variables, such as abundance, change over time, and in relation to each other. Historically, such models have been based on parametric equations that are derived from theory, and then tested in laboratory or simulation settings. However, there are many situations where explicit mathematical formulations are impractical: the exact mechanisms may be unknown or very complex; and the available data may be too limited or noisy to fully parameterize these equations.

These obstacles can hinder the effective usage of ecological models; for example, effort may be expended on increased monitoring and observation, yet yield little improvement in understanding or predictive power, because of inappropriate or inflexible mathematical hypotheses. In contrast, phenomenological models are highly flexible alternatives, which operate by inferring patterns and associations from the data (instead of relying on hypothesized equations). Here, we review the theoretical background for reconstructing dynamic rules from time series data, an appropriate known as "Empirical Dynamic Modeling (EDM)"" and the functionality of the `rEDM` package in R [XXX]. 

EDM combines the mathematical theory of attractor reconstruction [@Takens_1981; @Sauer_1991; @Casdagli_1991; @Deyle_2011] and simple methods for function approximation [@Sugihara_1990; @Sugihara_1994] in order to model complex systems using limited time series observations. Thus, it can be a powerful method for improving ecological modeling. Moreover, becasue EDM operates with minimal assumptions, it is particularly suitable for studying systems that exhibit non-equilibrium dynamics and nonlinear state-dependent behavior (i.e. where interactions change over time and as a function of the system state).

<!-- which are intended for nonlinear dynamic systems that can prove problematic for traditional modeling approaches. -->
<!-- especially problematic when used for predictive purposes () -->

<!-- Empirical models, which infer patterns and associations from the data (instead of using hypothesized equations), represent an alternative and highly flexible approach. Here, we review the theoretical background for empirical dynamic modeling (EDM) and the functionality of the `rEDM` package, which are intended for nonlinear dynamic systems that can prove problematic for traditional modeling approaches. -->

<!-- The basic goal underlying EDM is to reconstruct the behavior of dynamic systems using time series data. This approach is based on mathematical theory developed initially by [@Takens_1981], and subsequently expanded [@Sauer_1991; @Casdagli_1991; @Deyle_2011]. Because these methods operate with minimal assumptions, they are particularly suitable for studying systems that exhibit non-equilibrium dynamics and nonlinear state-dependent behavior (i.e. where interactions change over time and as a function of the system state). -->

# Empirical Dynamic Modeling

## Time Series as Observations of a Dynamic System

The essential concept underlying EDM is that time series are observations of a *dynamic system* over time - at each time point, the state of the dynamic system is represented as a point in a high-dimensional state space (Figure \@ref(fig:time-series-projection)). The axes of this space are the fundamental state variables, and the dynamical rules (governing equations) for the system describe how it evolves in time. For example, an experimental chemostate with one species of phytoplankton will have state variables for the population abundance and environmental factors, such as temperature, nutrients, and light. The dynamical rules for this system are the equations for growth and nutrient uptake, along with the experimental settings for temperature, light, and nutrient input.

Sequential observations comprise a time series of that state variable -- a projection from the high-dimensional state space to that particular coordinate axis. For example, in Figure \@ref(fig:time-series-projection), the states of the canonical Lorenz Attractor [-@Lorenz_1963] are projected to the $x$-axis, creating a time series of variable $x$. In many cases, each time series corresponds to a different state variable, but more generally a time series is an *observation function*, and can be any function of one or more state variables.

```{r time-series-projection, out.width = "80%", fig.cap = "Time Series Projection from the Lorenz Attractor", echo = FALSE}
knitr::include_graphics("figure_1.pdf")
```

A *conventional* mathematical model of a dynamical system requires knowledge of both the state variables and the equations. Uncertainty about the equations or vital parameters is then be addressed using model selection and methods for fitting the quations to observational data. However, it is quite common that there is structural uncertainty about the state variables or system equations (i.e. uncertainty about the identity of the variables or the form of the equations). In addition, observations are frequently incomplete or noisy, with insufficient data to fully parameterize the equations.

## Attractor Reconstruction / Takens' Theorem

EDM resolves these issues using the mathematical theory of attractor reconstruction [@Takens_1981; @Sauer_1991; @Casdagli_1991; @Deyle_2011]. Here, instead of requiring knowledge of all the state variables, lags of a time series are substitute for unknown or unobserved variables. For example, instead of using the state variables of $x$, $y$, and $z$ to depict the Lorenz Attractor (as in Figure \@ref(fig:time-series-projection)), we can instead use lags of $x$:
$$ \mathbf{x}_t = \left( x_t, x_{t-\tau}, \dots, x_{t-(E-1)\tau} \right) $$
where $E$ is the embedding dimension, or number of lags. The intuition is as follows: because time series are sequential observations of the system behavior, information about the dynamical rules is encoded in the data.

```{r attractor-reconstruction, out.width = "80%", fig.cap = "Attractor Reconstruction from 3 Lagged Coordinates", echo = FALSE}
knitr::include_graphics("figure_2.pdf")
```

This technique, known as *attractor reconstruction*, recovers the mathematical properties of the original system, provided that a sufficient number lags are used [@Takens_1981]. More specifically, the reconstructed states, $\mathbf{x}_t$ map uniquely to points in the original state space, and nearby points in the reconstruction correspond to nearby points in state space. Figure \@ref(fig:attractor-reconstruction) shows a reconstruction of the Lorenz attractor using 3 lags of variable $x$, demonstrating the correspondence between the reconstruction and the original Lorenz attractor.

// because of this unique mathematical property, the dynamical rules for the system are also preserved, allowing inference of how the variable used for reconstruction, e.g. x, changes in time. This can be applied directly to forecasting the time series used for reconstruction [], but also for identifying nonlinear behavior of the underlying system. More recently, these methods have been used for identification and quantification of causal effects [], and 
// dynamical rul

There are many applications for using this approach to recover system dynamics from time series. For example, empirical models can be used for forecasting [@Sugihara_1990], to understand nonlinear behavior [@Sugihara_1994], or to uncover mechanism [@Dixon_1999]. Recent work describes how EDM can be used to identify causal interactions, by testing whether two time series are observed from the same system [@Sugihara_2012]. 

<!-- In the next section, we demonstrate how the **rEDM** software package can be used to accomplish these tasks. -->

# The `rEDM` package

## Nearest Neighbor Forecasting using Simplex Projection

Takens' Theorem [-@Takens_1981] supports attractor reconstruction when a sufficient number of lags are used. Practically, the number of lags (the embedding dimension) is determined empirically using the data. Here, we use forecasting to determine the optimal embedding dimension. The reasoning here is that when the embedding dimension is too small (i.e. the number of lags is insufficient), the reconstruction will contain singularities -- different system states which map to the same point in the reconstruction. When this occurs, it is impossible to distinguish between different trajectories of the system, and forecast skill is diminished.

### Example

In this example, we demonstrate how to determine the embedding dimension using a nearest neighbor forecasting method, Simplex Projection [@Sugihara_1990]. Here, forecast skill is operationalized as the correlation between observed and predicted values. The time series come from a simulation of the tent map, a discrete-time dynamic system where a sequence, $x_t$, on the interval $[0, 1]$ is iterated according to:

\begin{equation*}
x_{t+1} = \begin{cases}
2x_t, & x_t < \frac{1}{2}\\
2(1-x_t), & x_t \ge \frac{1}{2}
\end{cases}
\end{equation*}

We begin by loading the **rEDM** package:
```{r load package}
library(rEDM)
```

and then the first differences of the tentmap time series:
```{r load tentmap data}
data(tentmap_del)
str(tentmap_del)
```

We can see that the data are contained in a single vector. Because the `simplex()` function accepts a numeric vector as the input data (see section XXX for more information about data formats), no further processing of the data is required.

```{r simplex on tentmap}
simplex_output <- simplex(time_series = tentmap_del, 
                          lib = c(1, 100), 
                          pred = c(201, 500))
```

Here, the `lib` and `pred` arguments specify the portions of the input data that are used to construct the model and for generating forecasts, respectively. `lib = c(1, 100)` indicates that rows 1 through 100 of the input data define the "library set" over which the method identified nearest neighbors for the simplex projection method [@Sugihara_1990]. `pred = c(201, 500)` indicates that rows 201 through 500 define the "prediction set" over which the models are run to produce forecasts.

The remaining arguments will be left at their default values (see section XXX for details). Notably, this means that the embedding dimension, $E$ will range from $1$ to $10$.

```{r simplex output}
str(simplex_output)
```

The output is a data.frame with rows for each separate model (in our case, the different values of the embedding dimension that were tried). The columns are model parameters and forecast statistics: for `simplex()`, the model parameters are `E`, embedding dimension; `tau`, time lag between successive dimensions; `tp`, time to prediction; and `nn`, number of nearest neighbors. The forecast statistics are `num_pred`, the number of predictions made; `rho`, Pearson's correlation coefficient between predictions and observations; `mae`, mean absolute error of predictions; `rmse`, root mean squared error of predictions; `perc`, the percent of predictions that are the same sign as observations; and `p_val`, the p-value for `rho` being significantly greater than 0, using Fisher's transformation [-@Fisher_1915]. For the purpose of comparison, the same forecast statistics are computed for a naive constant predictor (where the current value is propagated as the forecast).

```{r set up plot margins, include = FALSE}
par(mar = c(4, 4, 1, 1), mgp = c(2.5, 1, 0))
```

To determine the optimal embedding dimension, we plot `rho` (forecast skill) versus `E` (embedding dimension).

```{r rho vs E for tentmap}
plot(rho ~ E, data = simplex_output, type = "l",  
     xlab = "Embedding Dimension (E)", ylab = "Forecast Skill (rho)")
```

We observe that forecast skill peaks at `E = 2`, indicating that dynamics are best represented using 2 total lags of the `tentmap_del` time series. *Notably, this optimal embedding dimension does not necessarily correspond to the dimensionality of the original system.* Forecast skill is affected by factors such as observational noise, process error, and time series length. Thus, the optimal embedding dimension should be interpreted as a practical measure that is dependent on properties of the data.

## Identifying Nonlinearity

One concern is that statistically significant forecast skill can manifest as the result of temporal autocorrelation in the time series. We can distinguish between nonlinear deterministic dynamics and temporal autocorrelation (or red noise) using S-maps [@Sugihara_1994].

Whereas simplex projection uses nearest-neighbor interpolation to infer forecasts, the S-map forecasting method [@Sugihara_1994] uses local linear maps. In the S-map method, a different linear map is fitted separately for each point in the prediction space. This is achieved by weighting the points in the library set based on their distance from the target point in the reconstruction, thus allowing the coefficients of the linear map to vary. Here, the nonlinear tuning parameter, $\theta$, determines how the weights change as a function of distance. In the unique case of $\theta = 0$, weights do not change with distance, a single linear map is produced with every point in the library set given equal weighting. In contrast, when $\theta > 0$, nearby points receive larger weights, such that the local linear map is more sensitive to points that are nearby in the reconstruction.


Consequently, the coefficients do not change, and the model is equivalent to a global linear map -- effectively an autoregressive model.

Consequently, if the time series are sampled from autoregressive red noise, then there will be no improvement to forecast skill when $\theta > 0$. In fact, it is likely that the linear model ($\theta = 0$) produces the best forecasts, because it uses all the data for constructing the global linear map, thereby reducing the effects of observational noise. Thus, increases in forecast skill when $\theta > 0$ is indicative of nonlinear dynamics; allowing the local linear map to vary in state-space produces a better description of state-dependent behavior.

### Example

The S-map method is implemented as the function `s_map()` in the **rEDM** package. Following from the previous example, we again use the tent map time series, and set `E = 2` based on the results from simplex projection. Note that we allow many of the arguments to take on default values (e.g., `tau = 1`, `tp = 1`). If we had changed these for simplex projection, we would want to propagate them here. The default values for the nonlinear tuning parameter, `theta`, range from `0` to `8`, and are suitable for our purposes.

Note also, that the default value for `num_neighbors` is `0`. When using S-maps to test for nonlinear behavior, we want to use all points in the reconstruction, and allow `theta` to control the weighting assigned to individual points. By using `0` for this argument (an otherwise nonsensical value), the program will use all nearest neighbors.

```{r smap for tentmap}
smap_output <- s_map(time_series = tentmap_del, 
                     lib = c(1, 100), 
                     pred = c(201, 500), 
                     E = 2)
```

Again, the results are a data.frame with columns for each of the model parameters and forecast statistics, with rows for each run of the model. In this case, there is one run for each value of `theta`, so we can simply plot `theta` against `rho`:

```{r rho vs theta for tentmap}
plot(smap_output$theta, smap_output$rho, type = "l",
     xlab = "Nonlinearity (theta)", ylab = "Forecast Skill (rho)")
```

Here, we can see that forecast skill substantially improves as `theta` increases, indicating the presence of nonlinear dynamics. Typically, we would expect forecast skill to begin to decrease at high values of `theta`, because the local linear map will overfit to the nearest points. However, because the example data are observed without any noise, we continue to get a better approximation to the true function with higher `theta`.

By simulating the addition of some observational noise, we produce a plot that is more typical of real data:

```{r rho vs theta with noise}
ts_err <- tentmap_del + rnorm(length(tentmap_del), 
                              sd = sd(tentmap_del) * 0.2)
smap_output_err <- s_map(time_series = ts_err, 
                         lib = c(1, 100), 
                         pred = c(201, 500), 
                         E = 2)
plot(smap_output_err$theta, smap_output_err$rho, type = "l",
     xlab = "Nonlinearity (theta)", ylab = "Forecast Skill (rho)")
```

## Generalized Takens's Theorem

In addition to creating an attractor from lags of one time series, it is possible to combine different time series, if they are all observed from the same system [@Sauer_1991; @Deyle_2011]. The practical reality of complex dynamics, finite, noisy data, and stochastic drivers means that "multivariate" reconstructions can often be a better description than "univariate" reconstructions.

In **rEDM**, the `block_lnlp()` function generalizes the `simplex()` and `s_map()` functions: reconstructions can be formed from any set of coordinates, and forecasts are made using either of the two methods.

The data format for `block_lnlp()` is a matrix or data.frame, where each column is a separate time series, and each row is a temporal "slice" of the data comprised of simultaneous observations. A reconstruction is defined by specifying which columns to use as coordinates (the `columns` argument) and which column is to be forecast (the `target_column` argument). If lagged copies of a time series are desired as coordinates, they need to be pre-computed as separate columns in the input (e.g. via the `make_block()` function).

### Example

We begin by loading an example dataset from a coupled 3-species model system.

```{r load block_3sp data}
data(block_3sp)
str(block_3sp)
```

Here, the `block_3sp` variable is a 10-column data.frame with 1 column for time, and 3 columns for each of the variables (unlagged: $t$, lag-1: $t-1$, and lag-2: $t-2$). Note that the lagged columns begin with `NA` values because there are no observations of the variables for times $t < 1$. The vectors that include `NA` values are excluded if that specific value is needed for computation (see section "Data Input" for more details).

Columns can be referred to using either numerical indices or the column names. If numerical indices are used, `block_lnlp` has an option to indicate that the first column is actually a time index. When `first_column_time` is set to `TRUE`, a value of `1` for `target_column` will point to the first *data* column in the data.frame.

```{r block-lnlp for block-3sp, warning = FALSE}
lib <- c(1, NROW(block_3sp))
pred <- c(1, NROW(block_3sp))

cols <- c(1, 2, 4)
target <- 1

block_lnlp_output <- block_lnlp(block_3sp, lib = lib, pred = pred,
                                columns = cols, target_column = target,
                                stats_only = FALSE, first_column_time = TRUE,
                                silent = TRUE)
```

We can also refer to columns by the column names. This ignores the `first_column_time` argument when referring to columns, but does use the time column to label predictions:

```{r}
block_lnlp_output_2 <- block_lnlp(block_3sp, lib = lib, pred = pred,
                                  columns = c("x_t", "x_t-1", "y_t"), target_column = "x_t",
                                  stats_only = FALSE, first_column_time = TRUE,
                                  silent = TRUE)

# test for equality
stopifnot(identical(block_lnlp_output, block_lnlp_output_2))
```

Note that the default value for the `tp` argument is `1`, indicating that predictions are 1-step ahead (i.e. using each row of the `columns` variables to predict the subsequent row of the `target_column` variable). *In some cases, the data may be formatted to have the predicted variable aligned in the same row (but in a different column), and `tp` should be set to `0`.*

```{r}
str(block_lnlp_output)
```

By setting `stats_only` to `FALSE`, the output also includes the observed and predicted values in a list column (`model_output`). Unlike other columns, which are vectors of simple types (e.g. numeric, character), the `model_output` column is a list of length 1, whose single element is a data.frame of the observed and predicted values. If we had run multiple models, the `model_output` column would be a list of multiple data.frames (one data.frame of observed and predicted values for each model).

To compare the observed and predicted values, we want to pull out the appropriate columns:
```{r}
list_of_model_predictions <- block_lnlp_output$model_output
first_data_frame_of_predictions <- list_of_model_predictions[[1]]

observed <- first_data_frame_of_predictions$obs
predicted <- first_data_frame_of_predictions$pred
```

```{r, echo = FALSE}
par(pty = "s")
```

We can compare predicted vs observed in a square plot with a 1:1 line.

```{r observed vs predicted for block-lnlp, fig.width = 4, fig.height = 4}
plot_range <- range(c(observed, predicted), na.rm = TRUE)
plot(observed, predicted, xlim = plot_range, ylim = plot_range,
     xlab = "Observed", ylab = "Predicted", asp = 1)
abline(a = 0, b = 1, lty = 2, col = "blue")
```

```{r, echo = FALSE}
par(pty = "m")
```

## S-map Coefficients

As described in [@Deyle_2016], the S-map coefficients from the appropriate multivariate embedding can be interpreted as dynamic, time-varying interaction strengths. We demonstrate this approach for the same 3-species simulation as above, using `x`, `y`, and `z` as the coordinates to predict `x`.

```{r 3-species s-map coefficients example}
data(block_3sp)
lib <- c(1, 100)
pred <- c(101, 200)

cols <- c("x_t", "y_t", "z_t")
target <- "x_t"

block_smap_output <- block_lnlp(block_3sp, lib = lib, pred = pred,
                                columns = cols, target_column = target,
                                method = "s-map", theta = 2,
                                stats_only = FALSE, first_column_time = TRUE,
                                save_smap_coefficients = TRUE, silent = TRUE)
```

The `smap_coefficients` column of the output is a list-column with the data.frames for the S-map coefficients of each model. Since we have just have one model, we just want the first element of that list. The result is a data.frame with 200 rows (for each prediction) and 4 columns (for each of the 3 predictors and a constant).

```{r get coefficients}
smap_coeffs <- block_smap_output$smap_coefficients[[1]]
str(smap_coeffs)
```

Here, we plot the time series for the observed (solid) and predicted (dashed) values of `x` in the top panel; and the inferred interactions (s-map coefficients) for the influence of `x`, `y`, and `z` on future values of `x` in the remaining 3 panels.

```{r, echo = FALSE}
par(mfrow = c(4, 1), mar = c(2, 4, 1, 1), oma = c(0, 0, 0, 0),
    mgp = c(2.5, 1, 0))
```

```{r smap coefficients plot, fig.width = 6, fig.height = 7}
predictions <- block_smap_output$model_output[[1]]
t <- predictions$time

plot(t, predictions$obs, type = "l", col = "black", ylab = "x", xlab = "")
lines(t, predictions$pred, lty = 2)
legend("topright", legend = c("observed", "predicted"), lty = c(1, 2), bty = "n")

plot(t, smap_coeffs[, 1], type = "l", col = "red", ylab = "effect of x", xlab = "")
plot(t, smap_coeffs[, 2], type = "l", col = "blue", ylab = "effect of y", xlab = "")
plot(t, smap_coeffs[, 3], type = "l", col = "magenta", ylab = "effect of z", xlab = "")
```

```{r, echo = FALSE}
par(mfrow = c(1, 1), mar = c(4, 4, 1, 1))
```

## Causality Inference and Cross Mapping

One of the corollaries to Takens' Theorem is that multiple reconstructions not only map to the original system, but also to each other. Consider two variables, $x$ and $y$ that interact in a dynamic system. The univariate reconstructions based on lags of $x$ ($\mathbf{M}_x$) or $y$ ($\mathbf{M}_y$) are each capable of uniquely identifying system states. Thus, the reconstructed states of $\mathbf{M}_x$ map to reconstructed states of $\mathbf{M}_y$ (Figure \@ref(fig:cross-mapping)). This suggests a way to test whether $x$ and $y$ interact in the same system, by testing for a mapping between $\mathbf{M}_x$ and $\mathbf{M}_y$. Practically this is done by testing the predictive skill for the mapping from $\mathbf{M}_x$ to $y$ and from $\mathbf{M}_y$ to $x$.


```{r cross-mapping, out.width = "80%", fig.cap = "Cross Mapping Between Reconstructions of the Lorenz Attractor", echo = FALSE}
knitr::include_graphics("figure_3.pdf")
```

Furthermore, in the case of unidirectional causality, e.g. $x$ causes $y$, but $y$ does not cause $x$, we would only expect cross mapping to be successful in one direction. Somewhat counterintuitively, if a causal variable ($x$) leaves a signature on the affected variable ($y$), then it is possible to map from $\mathbf{M}_y$ to $\mathbf{M}_x$, but not vice-versa.

In essence, $\mathbf{M}_y$ must have complete information about $y$, which means it must include information about all its causes, including $x$. However, because $x$ behaves independently of $y$, the reconstruction $\mathbf{M}_x$ may be missing information about $y$, preventing a *complete* cross mapping from $\mathbf{M}_x$ to $y$.

To be more precise, although $x$ has incomplete information about $y$, it does have a causal influence on $y$, and there will likely be some predictive skill in the mapping from $\mathbf{M}_x$ to $y$. However, this will be limited to the statistical association between $x$ and $y$ and will generally not improve with more data.

In contrast, the mapping from $\mathbf{M}_y$ to $x$ is expected to become complete with more data. This convergence is a critical property for inferring causality, and can be tested by measuring the cross mapping skill when using different amounts of data to reconstruct $\mathbf{M}_y$. For a more detailed description of using cross mapping to infer causation, see [@Sugihara_2012; @Ye_2015a].

## Convergent Cross Mapping (CCM)

In **rEDM**, convergent cross mapping is implemented as the `ccm()` function, which provides a wrapper to compute cross map skill for different subsamples of the data. In the following example, we reproduce the analysis from [@Sugihara_2012] to identify causality between anchovy landings in California and Newport Pier sea-surface temperature. For this example, we use the previously identified embedding dimension of `E = 3`.

To identify convergence, we compute cross-map skill (Pearson's correlation, $\rho$ between observed and predicted values) over many random subsamples of the time series. The `lib_sizes` argument specifies the size of the library set, and `num_samples` specifies the number of subsamples generated at each library size. `random_libs` and `replace` specify how the subsamples will be generated. Here, setting both to `TRUE` enables random sampling with replacement.

```{r sardine anchovy ccm, warning = FALSE}
data(sardine_anchovy_sst)
anchovy_xmap_sst <- ccm(sardine_anchovy_sst, E = 3,
                        lib_column = "anchovy", target_column = "np_sst",
                        lib_sizes = seq(10, 80, by = 10), num_samples = 100,
                        random_libs = TRUE, replace = TRUE, silent = TRUE)
sst_xmap_anchovy <- ccm(sardine_anchovy_sst, E = 3,
                        lib_column = "np_sst", target_column = "anchovy",
                        lib_sizes = seq(10, 80, by = 10), num_samples = 100,
                        random_libs = TRUE, replace = TRUE, silent = TRUE)
str(anchovy_xmap_sst)
```

The output is a data.frame with statistics for each model run (in this case, 100 models at each of 8 library sizes). To interpret the results, we aggregate the cross map performance at each library size using the `ccm_means()` function, which computes a mean value at each unique `lib_size`. Because average cross map skill less than 0 means there is no prediction skill, (predictions should not be anticorrelated with observations), we set negative values to 0 when plotting.

```{r sardine anchovy ccm plot}
a_xmap_t_means <- ccm_means(anchovy_xmap_sst)
t_xmap_a_means <- ccm_means(sst_xmap_anchovy)

plot(a_xmap_t_means$lib_size, pmax(0, a_xmap_t_means$rho), type = "l", col = "red",
     xlab = "Library Size", ylab = "Cross Map Skill (rho)", ylim = c(0, 0.25))
lines(t_xmap_a_means$lib_size, pmax(0, t_xmap_a_means$rho), col = "blue")
legend(x = "topleft", legend = c("anchovy xmap SST", "SST xmap anchovy"),
       col = c("red", "blue"), lwd = 1, bty = "n", inset = 0.02, cex = 0.8)
```

# Authorsâ€™ contributions {-}

H.Y. and G.S. conceived the package; H.Y. designed the package, with assistance from A.T.C. and E.R.D. assisted on coding, testing, and documentation; H.Y., A.T.C., and E.R.D wrote the paper; All co-authors assisted with edits and approve publication.

# Acknowledgements {-}

**rEDM** is the latest incarnation of EDM code. Past versions have been developed by George Sugihara, Alan Trombla, Richard Penner, Victor Wong, Martin Casdagli, Jerome Cartagena, Mohsen Azarbayejani, Ava Pierce, Jennifer Trezzo, and Hao Ye.

We thank Jun Cai, Jane Cowles, Yair Daon, Andrew Edwards, Oliver Keyes, Steve Munch, James Stagge, Masayuki Ushio, and Ethan White, for their suggestions and contributions to the package and its documentation.

Package development and its documentation were supported by a Gordon and Betty Moore Foundation Data-Driven Discovery Initiative through Grant GBMF4563 (to Ethan P. White), NSF grant DEB-1655203 (GS), NSF grant DBI-1667584 (GS), U.S. Department of Defense Strategic Environmental Research and Development Program 15 RC-2509 (GS), Lenfest Ocean Program award 00028335 (GS), the Deutsche Bank-Jameson Complexity Studies Fund (GS), the Sugihara Family Trust (GS), the Leslie and John McQuown Gift and the McQuown Chair in Natural Sciences, UCSD (GS).

Data collection for the Cedar Creek LTER was funded by NSF grant DEB-9411972 (to G. David Tilman), DEB-0080382 (to G. David Tilman), DEB-0620652 (to G. David Tilman), and DEB-1234162 (to Eric Seabloom).

# Data accessibility {-}

Example datasets used here are included in the `rEDM` package, which is hosted at https://github.com/ha0ye/rEDM and also archived on Zenodo [@Ye_2018].

# Appendix

*Note that if the code detects any overlap in the lib and pred, it will prevent a vector from becoming its own neighbor by enabling leave-one-out cross-validation and outputting a warning message.*


# References
