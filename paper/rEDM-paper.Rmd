---
title: "`rEDM`: An **R** package for Empirical Dynamic Modeling and Convergent Cross Mapping"
preprint: false
author: 
  - name: Hao Ye
    affiliation: 1
    corresponding: true
    email: haoye@ufl.edu
  - name: Adam T. Clark
    affiliation: 2
  - name: Ethan R. Deyle
    affiliation: 3
  - name: George Sugihara
    affiliation: 3
affiliation:
  - code: 1
    address: Department of Wildlife Ecology and Conservation, University of Florida, 110 Newins-Ziegler Hall, PO Box 110430, Gainesville, FL USA 32611-0430
  - code: 2
    address: German Centre for Integrative Biodiversity Research (iDiv)
  - code: 3
    address: Scripps Institution of Oceanography, University of California San Diego, 9500 Gilman Drive MC 0202, La Jolla, CA USA 92093-0202
abstract: "
  1. Modeling the ecological processes that underlie time series observations can often be challenging because of uncertainty about the actual equations, or because governing equations depend on state variables that are unknown or unobserved.
  
  2. Empirical dynamic modeling (EDM) is an emerging framework for modeling dynamic systems, based on the mathematical theory of attractor reconstruction. These methods can be successful even when there is only one observed time series from a multivariate system.
  
  3. The `rEDM` package contains a number of functions for EDM, including methods for forecasting, inferring causal relationships among time series, and measuring changing interactions.
  
  4. `rEDM` facilitates the use of EDM approaches for understanding ecosystems, through the creation of models more robust to observations and structural uncertainty about equations, testing for causal relationships between observed variables, or exploratory analysis of time series datasets that can lead to hypothesis generation."
header-includes: >
  \usepackage{amsmath}
  \usepackage{setspace}
  \doublespacing
bibliography: refs.bib
output: 
  rticles::peerj_article:
    base_format: bookdown::pdf_document2 # for using \@ref()
    # base_format: rmarkdown::pdf_document 
---
### Keywords {-}
causal inference, empirical dynamics, forecasting, time series

### Running Title {-}
`rEDM`: Empirical Dynamic Modeling in R

\newpage

```{r figure defaults, include = FALSE}
knitr::opts_chunk$set(fig.width = 4.5, fig.height = 3)
```

# Introduction

In ecology, models are approximations of reality that are used for descriptive, explanatory, or predictive purposes.

One common application of models is to describe how important ecological variables, such as abundance, change over time, and in relation to each other. Historically, such models have been based on parametric equations that are derived from theory, and then tested in laboratory or simulation settings. However, there are many situations where explicit mathematical formulations are impractical: the exact mechanisms may be unknown or very complex; and the available data may be too limited or noisy to fully parameterize these equations.

These obstacles can hinder the effective usage of ecological models; for example, effort may be expended on increased monitoring and observation, yet yield little improvement in understanding or predictive power, because of inappropriate or inflexible mathematical hypotheses. In contrast, phenomenological models are highly flexible alternatives, which operate by inferring patterns and associations from the data (instead of relying on hypothesized equations). Here, we review the theoretical background for reconstructing dynamic rules from time series data, an approach known as "Empirical Dynamic Modeling (EDM)" and the functionality of the `rEDM` package in R [@Ye_2018]. 

EDM combines the mathematical theory of attractor reconstruction [@Takens_1981; @Sauer_1991; @Casdagli_1991; @Deyle_2011] and simple methods for function approximation [@Sugihara_1990; @Sugihara_1994] in order to model complex systems using limited time series observations. Thus, it can be a powerful method for improving ecological modeling. Moreover, becasue EDM operates with minimal assumptions, it is particularly suitable for studying systems that exhibit non-equilibrium dynamics and nonlinear state-dependent behavior (i.e. where interactions change over time and as a function of the system state).

<!-- which are intended for nonlinear dynamic systems that can prove problematic for traditional modeling approaches. -->
<!-- especially problematic when used for predictive purposes () -->

<!-- Empirical models, which infer patterns and associations from the data (instead of using hypothesized equations), represent an alternative and highly flexible approach. Here, we review the theoretical background for empirical dynamic modeling (EDM) and the functionality of the `rEDM` package, which are intended for nonlinear dynamic systems that can prove problematic for traditional modeling approaches. -->

<!-- The basic goal underlying EDM is to reconstruct the behavior of dynamic systems using time series data. This approach is based on mathematical theory developed initially by [@Takens_1981], and subsequently expanded [@Sauer_1991; @Casdagli_1991; @Deyle_2011]. Because these methods operate with minimal assumptions, they are particularly suitable for studying systems that exhibit non-equilibrium dynamics and nonlinear state-dependent behavior (i.e. where interactions change over time and as a function of the system state). -->

# Empirical Dynamic Modeling

## Time Series as Observations of a Dynamic System

The essential concept underlying EDM is that time series are sequential observations of a *dynamic system*. Briefly, a dynamic system consists of a *state space* and *dynamical rules* for how the system changes over time. At any point in time, the state of the system is represented as a point in the state space (Figure \@ref(fig:time-series-projection)), whose axes are the fundamental state variables. The rules for the system are typically represented as governing equations for how the system state will change as a function of the current state.

For example, an experimental chemostat setup might be represented with state variables for plankton abundance and the environmental factors of temperature, nutrients, and light. The dynamical rules are the equations for growth and nutrient uptake, and the experimental settings for temperature, light, and nutrient input.

The time series for a state variable can be generated by projecting from the state space to the corresponding coordinate axis. For example, in Figure \@ref(fig:time-series-projection), the states of the canonical Lorenz Attractor [-@Lorenz_1963] are projected to the $x$-axis, creating a time series of variable $x$. In many cases, different time series correspond to different state variables. More generally, however, a time series can be any function of one or more state variables (e.g. total plant cover doesn't distinguish individual species).

(ref:time-series-projection-caption) Projecting the system state of the Lorenz Attractor to the $x$-axis produces a time series of variable $x$.

```{r time-series-projection, out.width = "4.5in", fig.cap = '(ref:time-series-projection-caption)', echo = FALSE}
knitr::include_graphics("figure_1.pdf")
```

The conventional approach to modeling a dynamic system is to run the governing equations forward in time, and use observations of the state variables to determine the system state at different points in time. Uncertainty about vital parameters is addressed by fitting the equations to observational data; and uncertainty about the form of the equations is addressed with model selection. However, this approach assumes that the state variables are known, that they are observed sufficiently, and that the governing equations are known or can be reasonably approximated from theory. These assumptions can be problematic when modeling time series observed from complex, nonlinear systems.

## Attractor Reconstruction / Takens' Theorem

The EDM framework resolves these issues using the mathematical theory of attractor reconstruction [@Takens_1981; @Sauer_1991; @Casdagli_1991; @Deyle_2011]. The essential idea is to represent the state of the system with lags of a single time series, instead of all of the state variables. For example, in the canonical Lorenz Attractor, the state at time $t$ is nominally represented as $\left(x_t, y_t, z_t \right)$, where $x$, $y$, and $z$ are the state variables (Figure \@ref(fig:time-series-projection)). However, we can use an alternative represention consisting only of lags of $x$: $\mathbf{x}_t = \left( x_t, x_{t-\tau}, \dots, x_{t-(E-1)\tau} \right)$, where $E$ is the embedding dimension, or number of lags.

(ref:attractor-reconstruction-caption) Reconstruction of the Lorenz Attractor using 3 lags of x: $\mathbf{x}_t = \left( x_t, x_{t-\tau}, \dots, x_{t-(E-1)\tau} \right)$. The reconstruction maps smoothly to the original Lorenz Attractor (Figure \@ref(fig:time-series-projection)) and shows a clear visual correspondence.

```{r attractor-reconstruction, out.width = "4.5in", fig.cap = '(ref:attractor-reconstruction-caption)', echo = FALSE}
knitr::include_graphics("figure_2.pdf")
```

This approach, known as *attractor reconstruction*, preserves the mathematical properties of the original system, provided that a sufficient number lags are used [@Takens_1981]. More specifically, points in the reconstructed space, $\mathbf{x}_t$, map smoothly to points in the original state space ("smooth" meaning that nearby points in the reconstructed space also map to nearby points in the original state space). This relationship makes it possible to model a dynamic system using just a single time series.

For example, to describe the motion of $x$ in the Lorenz Attractor, we would ordinarily use a function of all of the state variables: $x_{t+1} = F\left(x_t, y_t, z_t\right)$. However, we can instead use an alternative formulation based on the reconstructed state space: $x_{t+1} = F^\prime\left(\mathbf{x}_t\right) = F^\prime\left(x_t, x_{t-\tau}, \dots, x_{t-(E-1)\tau} \right)$. Figure \@ref(fig:attractor-reconstruction) demonstrates this point with a reconstruction that uses 3 lags of $x$, which shows a visual correspondence with the original Lorenz attractor.

In practice, the application of this technique also requires selecting an appropriate time lag ($\tau$), embedding dimension ($E$), and methods for inferring $F^\prime$. We demonstrate how the `rEDM` software package can be used to accomplish these tasks, in the contexts of forecasting [@Sugihara_1990], testing for nonlinear behavior [@Sugihara_1994], and testing whether two time series variables belong to the same system and might be causally related [@Sugihara_2012]. 

# The `rEDM` package

## Selecting the embedding dimension using simplex projection

As mentioned previously, the mathematical theory of *attractor reconstruction* requires a sufficient number of lags [@Takens_1981]. If the number of lags is too few, then the reconstructed space ($\textbf{x}_t$) will not map smoothly to the original state space. Intuitively, we need enough lags so as to capture the influence of all of the state variables in the original state space (see also [@Whitney_1936]). Practically, the number of lags, or the embedding dimension, is determined empirically as a property of the data; we use forecast skill as the metric for selecting an optimal embedding dimension.

### Example

In this example, time series are generated from a simulation of the tent map, a discrete-time dynamic system where a sequence, $x_t$, on the interval $[0, 1]$ is iterated according to:

\begin{equation*}
x_{t+1} = \begin{cases}
2x_t, & x_t < \frac{1}{2}\\
2(1-x_t), & x_t \ge \frac{1}{2}
\end{cases}
\end{equation*}

The first difference of the tentmap time series are included with the package:

```{r load package}
library(rEDM)

data(tentmap_del)
str(tentmap_del)
```

To select the portions of the time series to be used for fitting the model and then testing the model, we define `lib` and `pred` variables:

```{r define lib and pred}
lib <- c(1, 100)
pred <- c(201, 500)
```

Here, `lib <- c(1, 100)` indicates that rows 1 through 100 of the input data are used for fitting the model, and `pred <- c(201, 500)` indicates that rows 201 through 500 of the input data are used for testing the model.

Since the time series come from a discrete map, a time lag of `tau = 1` is appropriate (and is the default parameter value). In addition, because we know the dynamics are fairly simple, the default range of $E = 1:10$ for the embedding dimension will suffice. More generally, in the absence of prior knowledge about the system, a reasonable range of values is 1 through $sqrt(n)$ with $n$ being the length of the time series [REF]).

```{r simplex on tentmap}
simplex_output <- simplex(time_series = tentmap_del, 
                          lib = lib, pred = pred, 
                          tau = 1, E = 1:10)
```

Here, the `simplex()` function makes forecasts, employing Simplex Projection to infer the dynamical model $F^\prime$ from the data using nearest neighbor approximation [@Sugihara_1990].

```{r simplex output}
str(simplex_output)
```

The output is a data.frame with rows for each individual model run, one row for each unique combination of model parameters. Here, there are `r NROW(simplex_output)` rows because we chose to vary the embedding dimension from 1 to 10. The first four columns are the model parameters: `E`, embedding dimension; `tau`, time lag between successive dimensions; `tp`, time to prediction; and `nn`, number of nearest neighbors. Next are the forecast statistics: `num_pred`, the number of predictions made; `rho`, Pearson's correlation coefficient between predictions and observations; `mae`, mean absolute error of predictions; `rmse`, root mean squared error of predictions; `perc`, the percent of predictions that are the same sign as observations; and `p_val`, the p-value for `rho` being significantly greater than 0, using Fisher's transformation [-@Fisher_1915]. For the purpose of comparison, the same forecast statistics are provided for a naive constant predictor, $\hat{x}_{t+tp} = x_t$.

(ref:simplex-caption) Forecast skill (rho) vs. Embedding Dimension (E) for the `tentmap_del` time series. A peak at E = 2 indicates that 2 total lags are optimal for reconstructing the dynamics. 

```{r tentmap-simplex, fig.cap = '(ref:simplex-caption)', echo = FALSE}
par(mar = c(4, 4, 1, 4), mgp = c(2.5, 1, 0))
plot(rho ~ E, data = simplex_output, type = "l",  
     xlab = "Embedding Dimension (E)", ylab = "Forecast Skill (rho)")
```

Figure \@ref(fig:tentmap-simplex) shows that forecast skill peaks at E = 2, indicating that the dynamics of the `tentmap_del` time series are best represented using 2 total lags. *Notably, this optimal embedding dimension does not necessarily correspond to the dimensionality of the original system.* Forecast skill is affected by factors such as observational noise, process error, and time series length. Thus, the optimal embedding dimension should be interpreted as a practical measure that is dependent on properties of the data.

## Identifying Nonlinearity

One concern is that forecast skill can occur because of temporal autocorrelation in the time series. In this approach to forecasting, we infer the function, $F^\prime$, that relates lags of a time series to its next value; consequently, this approach will also succeed when used to model temporally autocorrelated data (i.e. red noise). To distinguish between time series that contain deterministic nonlinear dynamics and red noise, we employ S-maps [@Sugihara_1994].

Whereas Simplex Projection approximates $F^\prime$ using nearest neighbors in the reconstruction space [@Sugihara_1990], the S-map uses local linear maps [@Sugihara_1994]. For each forecast, a local linear map is fitted, such that each point is assigned a weight depending on its distance (typically the Euclidean distance) to the point being forecast and the nonlinear tuning parameter, $\theta$. This allows the S-map to approximate any potential function, with $\theta$ controlling how nonlinear $F^\prime$ can be.

When $\theta = 0$, all points receive equal weights, and the local linear map is the same across the reconstruction space, making $F^\prime$ equivalent to a single global linear map. In contrast, for $\theta > 0$, nearby points receive larger weights, and the local linear map depends more on similar system states. Larger values of $\theta$ allow the forecast function, $F^\prime$ to be more "wiggly", similar to the weighting scheme in a LOESS [REF].

To distinguish between red noise and deterministic nonlinear dynamics, we evaluate how forecast skill depends on $\theta$. For autoregressived red noise, a single global map ($\theta = 0$) should produce the best forecasts, becauses it is fit to all of the data points, and therefore minimizes the effects of observational noise. However, for deterministic nonlinear dynamics, the true forecast function, $F^\prime$, will vary as a function of system state, and forecast skill will improve when $\theta > 0$.

### Example

The function `s_map()` implements the S-map method. Following from the previous example, we use the tent map time series, and fix `E = 2` based on the results from simplex projection. Similarly, the default values for `tau` and `tp` remain appropriate -- if we had used different values when determining the optimal embedding dimension, we would want to use the same values here. We also use the default set of values for `theta`, ranging from 0 to 8.

```{r smap for tentmap}
smap_output <- s_map(time_series = tentmap_del, 
                     lib = lib, pred = pred, 
                     E = 2)
```

Again, the result is a data.frame with rows for each model run, and columns for the model parameters and forecast statistics. Note that the default is `num_neighbors = 0`: this parameter controls how many of the nearest points the S-map is allowed to be fit to (which can ease calculations when there are a large number of data points). A value of 0 tells the S-map to use all the points.

(ref:smap-caption) Forecast skill (rho) vs. Nonlinearity (theta) for the `tentmap_del` time series. Increased forecast skill for theta > 0 suggests nonlinear dynamics in the data.

```{r tentmap-smap, fig.cap = '(ref:smap-caption)', echo = FALSE}
par(mar = c(4, 4, 1, 4), mgp = c(2.5, 1, 0))
plot(smap_output$theta, smap_output$rho, type = "l",
     xlab = "Nonlinearity (theta)", ylab = "Forecast Skill (rho)")
```

Figure \@ref(fig:tentmap-smap) shows that forecast skill substantially improves as $\theta$ increases, indicating the presence of nonlinear dynamics. Typically, we would also expect forecast skill to decrease as $\theta$ continues to increase, because the model will start to become overfit to individual points. However, because the data in this example are observed without any noise, the S-map continues to converge closer to the true function as $\theta$ increases.

If we simulate additive observational error and re-run the S-map analysis, the result is more typical of real data (Figure \@ref(fig:tentmap-smap-noise)).

```{r rho vs theta with noise}
ts_err <- tentmap_del + rnorm(length(tentmap_del), 
                              sd = sd(tentmap_del) * 0.2)
smap_output_err <- s_map(time_series = ts_err, 
                         lib = lib, pred = pred, 
                         E = 2)
```

(ref:smap-noise-caption) With added observational noise, forecast skill (rho) increases and then decreases with theta.

```{r tentmap-smap-noise, fig.width = 4.5, fig.height = 3, fig.cap = '(ref:smap-noise-caption)', echo = FALSE}
par(mar = c(4, 4, 1, 4), mgp = c(2.5, 1, 0))
plot(smap_output_err$theta, smap_output_err$rho, type = "l",
     xlab = "Nonlinearity (theta)", ylab = "Forecast Skill (rho)")
```

## Generalized Takens's Theorem

Rather than representing the system state with lags of a single time series [@Takens_1981], we can use different time series observed from the same system [@Sauer_1991; @Deyle_2011]. These *multivariate* representations can often be more effective when data are limited and noisy, or necessary when there are stochastic drivers in the system that need to be explicitly included. For more details, see @Casdagli_1991.

The `block_lnlp()` function generalizes the `simplex()` and `s_map()` functions: any combination of time series (and their lags) can be used, with either of the Simplex Projection or S-map methods. Instead of lags of one time series, the coordinates of the reconstruction must be manually specified as columns of the input data (`columns`); and similarly for the variable to be forecast (the `target_column` argument). If lags of a time series are desired as coordinates, they need to be pre-computed as additional columns (e.g. via the `make_block()` function).

### Example

We begin with an example dataset from a coupled 3-species model system.

```{r load block_3sp data}
data(block_3sp)
str(block_3sp)
```

`block_3sp` is a 10-column data-frame, consisting of `time`, and 3 lags of each of the variables: unlagged (`_t-1`), lag-1 (`_t-1`), and lag-2 (`_t-2`). Note that the lagged columns begin with `NA` values because the entries correspond to missing observations of the variables (e.g. `x` at time 0 or -1). Points that contain missing values are automatically excluded from fitting and forecasting.

Columns can be referred to by numerical index (starting with 1) or column name. The optional argument `first_column_time` is used to indicate that the input data has a time index in the first column is a time index, and should be skipped. For example, supposed that we want to use the coordinates $\left(x_t, x_{t-1}, y_t\right)$ to predict future values of $x_t$. To properly ignore the time column, the coordinates of the reconstruction correspond to `columns = c(1, 2, 4)` and `target_column = 1` with `first_column_time = TRUE`.

```{r block-lnlp for block-3sp, warning = FALSE}
lib <- c(1, 100)
pred <- c(101, 200)

block_lnlp_output <- block_lnlp(block_3sp, lib = lib, pred = pred,
                                columns = c(1, 2, 4), 
                                target_column = 1,
                                first_column_time = TRUE,
                                stats_only = FALSE)
```

We can also refer to columns by name. Here, specifying `first_column_time = TRUE` is unnecessary, helps in labeling the predictions with the correct time value.

```{r block-lnlp with alternative column reference}
block_lnlp_alt <- block_lnlp(block_3sp, lib = lib, pred = pred,
                             columns = c("x_t", "x_t-1", "y_t"), 
                             target_column = "x_t", 
                             first_column_time = TRUE, 
                             stats_only = FALSE)

# test for equality
stopifnot(identical(block_lnlp_output, block_lnlp_alt))
```

As in `simplex()` and `s_map()`, the default value for the `tp` argument is 1---predictions are 1-step ahead. That is, the forecast model is of the form $F^\prime: \left(x_t, x_{t-1}, y_t\right) \rightarrow x_{t+\texttt{tp}}$) *In some cases, the data may be formatted such dependent variable is a different column in the same row, and so `tp` should be set to 0.*

```{r block-lnlp output}
str(block_lnlp_output)
```

With `stats_only = FALSE`, the output also includes a list column (`model_output`), where each element is a data.frame that contains observed and predicted values for that model run. We can extract out these values to inspect how individual predictions line up against observed values.

```{r extract block-lnlp predictions}
list_of_model_predictions <- block_lnlp_output$model_output
first_model_predictions <- list_of_model_predictions[[1]]

observed <- first_model_predictions$obs
predicted <- first_model_predictions$pred
```

(ref:block-pred-vs-obs-caption) The predictions of variable $x$ against the observed values of $x$ from running `block_lnlp()`. Perfect predictions will fall on the 1:1 line (blue dashed line).

```{r block-pred-vs-obs, fig.width = 4, fig.height = 4, fig.cap = '(ref:block-pred-vs-obs-caption)', echo = FALSE}
par(pty = "s")
plot_range <- range(c(observed, predicted), na.rm = TRUE)
plot(observed, predicted, xlim = plot_range, ylim = plot_range,
     xlab = "Observed", ylab = "Predicted", asp = 1)
abline(a = 0, b = 1, lty = 2, col = "blue")
```

## S-map Coefficients

As described in @Deyle_2016, the S-map coefficients can be interpreted as dynamic, time-varying interaction strengths, given an appropriate choice of coordinates for reconstruction. We demonstrate this using $\left(x_t, y_t, z_t\right)$ to predict $x_{t+1}$ in the 3-species simulation described previously.

```{r 3-species s-map coefficients example}
data(block_3sp)
lib <- c(1, 100)
pred <- c(101, 200)

block_smap <- block_lnlp(block_3sp, lib = lib, pred = pred,
                         columns = c("x_t", "y_t", "z_t"), 
                         target_column = "x_t",
                         method = "s-map", theta = 2,
                         stats_only = FALSE, 
                         first_column_time = TRUE,
                         save_smap_coefficients = TRUE)
```

As with `model_output`, the `smap_coefficients` column is a list-column, where the elements are data.frames containing the S-map coefficients of each model.

```{r get coefficients}
smap_coeffs <- block_smap$smap_coefficients[[1]]
str(smap_coeffs)
```

(ref:smap-coeffs-caption) **A.** Time series for $x$ compared with predictions (points). **B-D.** The inferred effects of $x$, $y$, and $z$ from the S-map model that predicts future values, $x_{t+1}$.

```{r smap-coeffs, fig.width = 4.5, fig.height = 5, fig.cap = '(ref:smap-coeffs-caption)', echo = FALSE}
par(mfrow = c(4, 1), mar = c(2, 4, 1, 4), oma = c(2, 0, 0, 0),
    mgp = c(2.5, 1, 0), yaxs = "i", xaxs = "i")

cols <- viridis::viridis(4, option = "D")
predictions <- block_smap$model_output[[1]]
make_panel <- function(y, col = "black", ..., label = "A")
{
    plot(predictions$time, y, xlim = c(100, 200), ylim = c(-2, 2), 
         type = "l", col = col, xlab = "", lwd = 1.5, ...)
    mtext(label, side = 3, at = 88, line = -0.2)
}

make_panel(predictions$obs, ylab = "x", col = "gray")
points(predictions$time, predictions$pred, lty = 2, pch = 3)
legend(x = 200, y = 2, legend = "predictions", pch = 3, bty = "n", 
       xjust = 1, yjust = 0.75, adj = c(0, 0.5), xpd = TRUE)

make_panel(smap_coeffs[, 1], col = cols[1], ylab = "effect of x", label = "B")
make_panel(smap_coeffs[, 2], col = cols[2], ylab = "effect of y", label = "C")
make_panel(smap_coeffs[, 3], col = cols[3], ylab = "effect of z", label = "D")
mtext("Time", side = 1, outer = TRUE, line = 0.5, cex = 0.8)
```

Here, `smap_coeffs` is a data.frame with 100 rows (the number of predictions) and 4 columns (the number of predictors plus one for a constant). Figure \@ref(fig:smap-coeffs) shows the observed and predicted values of `x_{t+1}` and the influence of `x_t`, `y_t`, and `z_t`, respectively, on the predictions.

## Causal Inference and Cross Mapping

One corollary to Takens' Theorem [-@Takens_1981] is that smooth maps also exist between multiple reconstructions of the same system. Consider a dynamic system that contains two interacting variables, $x$ and $y$. The univariate reconstructions, based on lags of $x$ ( $\mathbf{x}_t = \left( x_t, x_{t-\tau}, \dots, x_{t-(E_x-1)\tau} \right)$) and $y$ ( $\mathbf{y}_t = \left( y_t, y_{t-\tau}, \dots, y_{t-(E_y-1)\tau} \right)$) respectively, are each capable of uniquely identifying system states. Thus, there is also a smooth map between $\mathbf{x}_t$ and $\mathbf{y}_t$ (Figure \@ref(fig:cross-mapping)). In practical terms, we can test for the map between $\mathbf{x}_t$ and $\mathbf{y}_t$ by measuring the predictive skill of mapping from $\mathbf{x}_t$ to $y_t$ and from $\mathbf{y}_t$ to $x_t$.

(ref:cross-mapping-caption) Two reconstructions of the Lorenz Attractor. The one on the left uses lags of $x$ ( $\mathbf{x}_t = \left( x_t, x_{t-\tau}, \dots, x_{t-(E_x-1)\tau} \right)$) and the one on the right uses lags of $y$ ( $\mathbf{y}_t = \left( y_t, y_{t-\tau}, \dots, y_{t-(E_y-1)\tau} \right)$). There is a smooth map between the two reconstructions, which also show a visual correspondence with each other.

```{r cross-mapping, out.width = "4.5in", fig.cap = '(ref:cross-mapping-caption)', echo = FALSE}
knitr::include_graphics("figure_3.pdf")
```

### Directionality

When causation occurs in only one direction, e.g. $x$ influences $y$, but $y$ does not influence $x$, the mapping between $\mathbf{x}_t$ and $\mathbf{y}_t$ is only assured in one direction. Specifically, if $x$ influences $y$, then there is a mapping from $\mathbf{y}_t$ to $x_t$ (i.e. from the affected variable to the causal variable). Although the direction is counterintuitive, this occurs because $\mathbf{y}_t$ must capture all of the dynamical rules about how $y$ changes in time, including the influence of $x$. Consequently, $\mathbf{y}_t$ will be able to recover the values of $x$. Conversely, $\mathbf{x}_t$ is only guaranteed to capture the dynamical rules for how $x$ changes in time, and this does not require any information about $y$. Thus, there is no guarantee of a mapping from $\mathbf{x}_t$ to $y_t$.

Nevertheless, because $x$ does influence $y$, there will be some predictive skill in the mapping from $\mathbf{x}_t$ to $y_t$. However, except in cases where $y$ is synchronized to $x$ [@Rulkov_1995], the quality of the mapping will be limited, regardless of the amount of data. In contrast, the mapping from $\mathbf{y}_t$ to $x_t$ will continue to improve in skill with more and more data. This convergence is a critical property for inferring causality, and can be tested by measuring the cross mapping skill with larger and larger subsamples of the data. For a more detailed description of using cross mapping to infer causation, see @Sugihara_2012 and @Ye_2015a.

## Convergent Cross Mapping (CCM)

Convergent cross mapping is implemented as the `ccm()` function, which provides a wrapper to compute cross map skill for different sized subsamples of the data. In the following example, we reproduce the analysis from [@Sugihara_2012] to identify causality between anchovy landings in California and Newport Pier sea-surface temperature.

To identify convergence, we compute cross-map skill (Pearson's correlation, $\rho$ between observed and predicted values) over many random subsamples of the time series. The `lib_sizes` argument specifies the size of the library set, and `num_samples` specifies the number of subsamples at each library size. `random_libs` and `replace` specify how the subsamples will be generated. Here, setting both to `TRUE` enables random sampling with replacement. We use `E = 3`, following the supplementary materials for @Sugihara_2012.

```{r sardine-sst-cross-mapping}
data(sardine_anchovy_sst)
sardine_sst <- ccm(sardine_anchovy_sst, E = 3,
                   lib_column = "sardine", target_column = "sio_sst",
                   lib_sizes = seq(10, 80, by = 10), num_samples = 100,
                   random_libs = TRUE, replace = TRUE, silent = TRUE)
sst_sardine <- ccm(sardine_anchovy_sst, E = 3,
                   lib_column = "sio_sst", target_column = "sardine",
                   lib_sizes = seq(10, 80, by = 10), num_samples = 100,
                   random_libs = TRUE, replace = TRUE, silent = TRUE)
str(sardine_sst)
```

As before, the output is a data.frame with statistics for each model run (in this case, 100 models at each of 8 library sizes). To interpret the results, we use the `ccm_means()` function to compute a mean value of the forecast statistics at each unique library size. 

```{r sardine-sst-ccm-means}
sardine_sst_ccm_means <- ccm_means(sardine_sst)
sst_sardine_ccm_means <- ccm_means(sst_sardine)
str(sardine_sst_ccm_means)
```

Figure \@ref(fig:sardine-sst) shows the cross mapping between California sardine landings and Scripps Pier sea-surface temperature. Here, cross-map skill improves with larger library size for sardines cross mapping to SST, suggesting an influence of temperature on sardines. As expected, in the opposite direction, there is no evidence that the sardines influence sea-surface temperature.

(ref:sardine-sst-caption) Cross mapping between California sardine landings and Scripps Pier sea-surface temperature suggests an influence of temperature on sardines, but not vice-versa.

```{r sardine-sst, fig.cap = '(ref:sardine-sst-caption)', echo = FALSE}
par(mar = c(4, 4, 1, 4), mgp = c(2.5, 1, 0))

plot(sardine_sst_ccm_means$lib_size, pmax(0, sardine_sst_ccm_means$rho), 
     type = "l", col = "red",
     xlab = "Library Size", ylab = "Cross Map Skill (rho)", ylim = c(0, 0.4))
lines(sst_sardine_ccm_means$lib_size, pmax(0, sst_sardine_ccm_means$rho), 
      col = "blue")
legend(x = "topleft", legend = c("sardine xmap SST", "SST xmap sardine"),
       col = c("red", "blue"), lwd = 1, cex = 0.8)
```

# Data Formats

`rEDM` is designed to work with commonly used data structures. Functions that operate on single-variable time-series data (e.g. `simplex()`, `s_map()`) will accept native R vectors or `ts` time-series objects. Functions that operate on multivariate time-series data (e.g. `block_lnlp()`, `ccm()`) will accept native R matrices, data.frames, or `mts` time-series objects. Please see the documentation associated with individual functions for more details.

Missing data can be recorded using `NA` or `NaN` values. These missing values are automatically ignored as appropriate. For example, when using Simplex Projection, nearest neighbors must be complete vectors, and reconstructed vectors that contain missing coordinates will be ignored.

Note that in the absence of an observed target value, it is still possible to make a forecast if there is a complete set of predictor variables. Thus it is possible to forecast unobserved (future) states. This can be accomplished by substituting NA for unknown future values. However, note that these forecasts are not subsequently re-used as input for multi-step-ahead predictions.

# Conclusion




Chang_2017


# Authorsâ€™ contributions {-}

H.Y. and G.S. conceived the package; H.Y. designed the package, with assistance from A.T.C. and E.R.D. on coding, testing, and documentation; All authors were involved with writing and editing the paper.

# Acknowledgements {-}

`rEDM` is an implementation of EDM code as an R package, and benefits from past versions developed by George Sugihara, Alan Trombla, Richard Penner, Victor Wong, Martin Casdagli, Jerome Cartagena, Mohsen Azarbayejani, Ava Pierce, Jennifer Trezzo, and Hao Ye.

We thank Jun Cai, Jane Cowles, Yair Daon, Andrew Edwards, Oliver Keyes, Steve Munch, James Stagge, Masayuki Ushio, and Ethan White, for their suggestions and contributions to the package.

This project is supported by a Gordon and Betty Moore Foundation Data-Driven Discovery Initiative through Grant GBMF4563 (to Ethan P. White), NSF grant DEB-1655203 (GS), NSF grant DBI-1667584 (GS), U.S. Department of Defense Strategic Environmental Research and Development Program 15 RC-2509 (GS), Lenfest Ocean Program award 00028335 (GS), the Deutsche Bank-Jameson Complexity Studies Fund (GS), the Sugihara Family Trust (GS), the Leslie and John McQuown Gift and the McQuown Chair in Natural Sciences, UCSD (GS).

Data collection for the Cedar Creek LTER was funded by NSF grant DEB-9411972 (to G. David Tilman), DEB-0080382 (to G. David Tilman), DEB-0620652 (to G. David Tilman), and DEB-1234162 (to Eric Seabloom).

# Data accessibility {-}

Example datasets used here are included in the `rEDM` package, which is hosted at https://github.com/ha0ye/rEDM and also archived on Zenodo [@Ye_2018].

# Appendix

*Note that if the code detects any overlap in the lib and pred, it will prevent a vector from becoming its own neighbor by enabling leave-one-out cross-validation and outputting a warning message.*


# References
